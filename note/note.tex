%        File: notes.tex
%     Created: Thu Nov 07 11:00 AM 2013 C
% Last Change: Thu Nov 07 11:00 AM 2013 C
%
\documentclass[a4paper]{article}

\include{preamble}

\addbibresource{conflim.bib}

\newcommand*\Hnull{\maybebm{\mathrm{H}_0}\xspace}
\newcommand*\Prob{\mathrm{Pr}}
\newcommand*\PG{\maybebm{P_\mathrm{G}}\xspace}
\newcommand*\Pchi{\maybebm{P_{\chi^2}}\xspace}
\newcommand*\xm{\maybebm{x_{\mathrm{m}}}\xspace}
\newcommand*\ta{\maybebm{c_\alpha}\xspace}
\newcommand*\ISF{\mathrm{ISF}}
\newcommand*\QF{\mathrm{QF}}

\title{A Semi-analytic Solution for Unified Confidence Intervals Based
on Critical Likelihood-ratio Values}
\author{Jan Eike von Seggern}
\begin{document}
\maketitle

\section{Introduction}

\textcite{Feldman1998} proposed to construct Frequentist confidence
intervals
using an ordering scheme based on likelihood ratios. This method is
known as the ``Unified Method''. It allows to avoid so-called
flip-flopping between quoting one- and two-sided intervals depending on
the measured value, which is a source of significant undercoverage.

In their original paper, \citeauthor{Feldman1998} use the classical
Neyman prescription to construct confidence belts, which are
subsequently used to derive confidence intervals based on a measured
value. However, the construction confidence intervals is tightly
connected to statistical hypothesis
testing\cite{wiki:confidenceinterval,Cox1974}: The confidence intervals
can be constructed by including all values for which a test statistic is
above a critical value\cite{Sen2009}.

In this note, we will present a semi-analytical method to construct
unified confidence intervals for the case of estimating the expectation
value, $\mu$, for a Gaussian-distributed observable with known variance,
$\sigma^2$, and constraint $\mu \geq 0$. In contrast to the classical
Neyman procedure, this method does not rely on constructing a confidence
belt.

\section{Confidence Intervals and Hypothesis Testing}

As mentioned above, the construction of confidence intervals is tightly
connected to hypothesis testing: Consider estimating the parameter
$\theta$ of a single-parameter distribution, $f(x; \theta)$, \eg the expectation value
of a Gaussian distribution. The confidence interval (or region) for
$\theta$ with confidence level \CL can be constructed to be
the set of values $\{\theta\}$ for which the null hypothesis
%
\begin{equation*}
  \Hnull: \theta = \theta_\mathrm{true}
\end{equation*}
%
is not rejected with significance $\alpha = 1- \CL$ for a measured value
of \xm.

Let $t(x)$ be a test statistic bound to the positive domain, $t(x) \geq
0$, with mode at $t(x) = 0$ if \Hnull is true. Then, \Hnull is rejected
if $t(\xm)$ is in the critical region, defined by
%
\begin{equation}
  \label{eq:hnull:rejection}
  t(\xm) \geq \ta
  \fcomma
\end{equation}
%
where the critical value \ta is defined by
%
\begin{equation}
  \label{eq:critical:value:definition}
  \Prob\left[ t(x) \geq \ta; x \sim f(\theta)\right] = \alpha
  \fcomma
\end{equation}
%
\ie the probability to observe a sample $x$ with $t(x) > \ta$ is
$\alpha$ if the parameter of the distribution has the value $\theta$.%
\footnote{%
  Alternatively, one can recast \Eqs{eq:hnull:rejection} and
  \eqref{eq:critical:value:definition}: \Hnull is accepted if
  $t(\xm) \geq \ta$
  with \ta given by
  $\Pr\left[ t(x) < \ta; x \sim f(\theta) \right] = \CL = 1 - \alpha$.
}%
Hence, for given $\theta$, the critical value \ta is given by the
$1-\alpha$ quantile of the distribution of $t(x)$ and, thus, a function of
$\theta$,
%
\begin{equation*}
  \ta(\theta) = \QF_t(1-\alpha; \theta)
  \fcomma
\end{equation*}
%
where the parameter $\theta$ is written explicitly to emphasize that the
quantile function of the distribution of $t(x)$, $\QF_t$, is evaluated assuming \Hnull is true.

\paragraph{}
In order to construct unified confidence intervals, the likelihood ratio
%
\begin{equation*}
  R(\theta; x) = \frac{L(\theta; x)}{L(\hat{\theta}(x); x)}
\end{equation*}
is used as test statistic\cite{Feldman1998}, where $L$ is the likelihood function and
$\hat{\theta}(x)$ maximizes $L$ for given $x$ obeying constraints (\eg
$\theta \geq 0$).
Hence, $R$ takes up values in
$[0,1]$. Since $\hat{\theta}(x)$ maximizes $L$, the mode of $R$ under
\Hnull is 1 and \Eqs{eq:hnull:rejection} and
\eqref{eq:critical:value:definition} need to be reformulated: 
The critical region is know given by
%
\begin{equation}
  \label{eq:hnull:rejection:likelihood:ratio}
  R(\theta; \xm) \leq R_\alpha(\theta)
\end{equation}
%
with critical value $R_\alpha(\theta)$ defined by 
%
\begin{equation}
  \label{eq:critical:value:definition:likelihood:ratio}
  \Pr\left[ R(\theta; x) < R_\alpha(\theta); x \sim f(\theta)  \right] = \alpha
  \fcomma
\end{equation}
%
i.e.~the $\alpha$-quantile of the distribution of the likelihood ratio,
%
\begin{equation*}
  R_\alpha(\theta) = \QF_R(\alpha; \theta)
  \fstop
\end{equation*}
%


\section{Likelihood-ratio Distribution for Constrained Gaussian}

As seen above, it is necessary to calculate the $\alpha$-quantile of the
likelihood-ratio distribution. Therefore, it is necessary to know the
distribution of the likelihood-ratio, which will be derived in this
section for the case of the mean of a Gaussian that is constrained to
the positive domain. For a single measurement, $x$, the
maximum-likelihood estimator is given by\cite{Feldman1998}
%
\begin{equation*}
  \hat\mu(x) = \max(0, x)
  \fstop
\end{equation*}
%
Hence, the likelihood ratio is given by (Eq.~(4.3) of
\cite{Feldman1998})
%
\begin{equation*}
  \Lambda(\mu; x) = 
  \begin{cases}
    \displaystyle
    \exp\left[ - \frac{1}{2} \left( x - \mu \right)^2 \right] & \text{if } x \geq 0
    \fcomma
    \\
    \displaystyle
    \exp\left[ \mu \left( x - \frac{\mu}{2} \right) \right] &
    \text{otherwise.}
  \end{cases}
\end{equation*}
%

For convenience, we will use $\lambda=-2\ln\Lambda$ instead,
%
\begin{equation}
  \label{eq:log:likelihood:ratio}
  \lambda(\mu; x) = 
  \begin{cases}
    \displaystyle
     (x-\mu)^2 & \text{if } x \geq 0
    \fcomma
    \\
    \displaystyle
    \mu \left( \mu - 2x \right) & \text{otherwise,}
  \end{cases}
\end{equation}
%
which takes up values from the positive domain and has mode 0.
Accordingly, \Eqs{eq:hnull:rejection} and
\eqref{eq:critical:value:definition} can be used.

The cumulative distribution function (CDF) for $\lambda$ is given by
\begin{align*}
  P_\lambda(l) & = \Prob(\lambda < l; \mu) \\
  & =
  \underbrace{\Prob(\lambda < l \wedge x \geq 0; \mu)}_{P_+(l)}
  +
  \underbrace{\Prob(\lambda < l \wedge x < 0; \mu)}_{=P_-(l)}
  \fcomma
\end{align*}
where the contributions $P_\pm$ correspond to the two cases of
\Eq{eq:log:likelihood:ratio} and $\mu$ is mentioned explicitly to
remind that the probabilities are evaluated under the assumption that
the expectation value of the distribution of $x$ is $\mu$.

\paragraph{$\maybebm{P_+}$:}
The probability contribution $P_+$ can be rewritten using conditional
probabilities,
%
\begin{align}
  P_+(l)
  & = \Prob(\lambda < l \wedge x \geq 0; \mu)
  \nonumber
  \\
  & = \Prob(\lambda < l | x \geq 0) \cdot \Prob(x \geq 0; \mu)
  \label{eq:p:plus:conditional}
  \fcomma
\end{align}
%
where $\Prob(x \geq 0; \mu)$ is given by the Gaussian CDF, \PG, by
%
\begin{equation}
  \label{eq:p:plus:prior}
  \Prob(x \geq 0; \mu) = 1 - \PG(0; \mu, \sigma)
  \fstop
\end{equation}
%

Because $x$ is Gaussian-distributed, 
$\lambda = [(x-\mu)/\sigma]^2$ follows in principle a
chi-squared distribution with one degree of freedom. However, due to the
constraint $x \geq 0$, we find
%
\begin{equation*}
  \frac{x-\mu}{\sigma} > -\frac{\mu}{\sigma}
\end{equation*}
%
and the parabola $\lambda=[(x-\mu)/\sigma]^2$ is realized only once
for values above $(\mu/\sigma)^2$ but twice for values below this
bound. Hence, the chi-squared CDF, \Pchi, has to be re-weighted
in order to calculate $P_+$. The factor for re-weighting is given by
\begin{equation*}
  \overbrace{2 \cdot \Pchi[(\mu/\sigma)^2]}^{\text{contribution
  for} \lambda < (\mu/\sigma)^2} +
  \overbrace{
    \underbrace{\Pchi(\infty)}_{=1} - \Pchi[(\mu/\sigma)^2]
}^{\text{contribution
  for} \lambda \geq (\mu/\sigma)^2}
  = 1 + \Pchi[(\mu/\sigma)^2]
  = 1 + \nu
     \fcomma
\end{equation*}
and the conditional probability in \Eq{eq:p:plus:conditional} is given
by
%
\begin{equation}
  \label{eq:p:plus:conditional:only}
  \Prob(\lambda < l | x \geq 0) =
  \begin{cases}
    \displaystyle
    \frac{2 \cdot \Pchi(l)}{1+\nu} & \text{if } l < (\mu/\sigma)^2
    \\
    \displaystyle
    \frac{\Pchi(l) + \nu}{1 + \nu} & \text{otherwise.}
  \end{cases}
\end{equation}
%

\paragraph{$\maybebm{P_-}$:}
For $x<0$, $\lambda$ is a linear function of $x$ and
%
\begin{equation*}
  \lambda < l
  \quad \Leftrightarrow \quad
  x > \xi = \frac{\sigma^2}{2\mu} \left( \frac{\mu^2}{\sigma^2} - l \right)
  \fstop
\end{equation*}
%
Hence, we find for the probability contribution $P_-$
%
\begin{align}
  P_-(l)
  & =
  \Pr(\lambda < l \wedge x < 0; \mu)
  \nonumber
  \\
  & =
  \Pr(\xi < x < 0; \mu)
  \nonumber
  \\
  & =
  \begin{cases}
    \displaystyle
    0 & \text{if } l < (\mu/\sigma)^2 \fcomma \\
    \displaystyle
    \PG(0; \mu, \sigma) - \PG(\xi; \mu, \sigma) & \text{otherwise} \fstop
  \end{cases}
  \label{eq:p:minus:result}
\end{align}
%

\paragraph{}
As numerical approximations for the CDFs of chi-squared and Gaussian
distributions are readily available in many programming libraries,
$P_\lambda$ can be easily computed by combining \Eqs{eq:p:plus:prior},
\eqref{eq:p:plus:conditional:only} and \eqref{eq:p:minus:result} and the
critical $\lambda$-values, $\lambda_\alpha$, can be approximated by, for
example, a bisection algorithm to finally calculate the critical
likelihood-ratio value $c_\alpha = \exp(-2\lambda_\alpha)$.

\printbibliography[heading=bibintoc]

\end{document}


